# 项目介绍 

CIFAR-10分类

## 综述

​		本项目使用一个经典的数据集cifar-10进行分类任务。该数据集包括60000张32 x 32的彩色图像，其中训练集50000张，测试集10000张。cifar-10一共标注为10类，每一类图片6000张。这10类分别是airplane(飞机)，automobile（汽车），bird（鸟），cat（猫），deer（鹿），dog（狗），frog（青蛙），horse（马），ship（船）和truck（卡车），其中没有任何的重叠情况，即airplane只包括飞机，automobile只包括小型汽车，也不会在同一张照片中出现两类事物。

如下图所示，列举了以上十个类以及它们的十张图：

<img src="README.assets/10.png" alt="10"  />

数据集的下载路径：http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

## 具体步骤

1. 使用torchvision加载并预处理CIFAR-10数据集：CIFAR-10^3是一个常用的彩色图片数据集，它有10个类别: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'。每张图片都是$3\times32\times32$，也即3-通道彩色图片，分辨率为$32\times32$。
2. 定义网络 ：构建LeNet网络，修改self.conv1第一个参数为3通道，因CIFAR-10是3通道彩图。
3. 定义损失函数和优化器：定义(loss和optimizer)。
4. 训练网络并更新网络参数 ：所有网络的训练流程都是类似的，不断地执行如下流程：输入数据、前向传播+反向传播、更新参数。
5. 测试网络：得到测试结果。

## 使用工具

1. Python 3.6+
2. torch1.0
3. numpy
4. torchvision
5. matplotlib

## 超参数的变化

​		本项目将Learn_rate设为超参数，对不同的Learn_rate下的损失率进行比较，首先将Learn_rate设为0，003时Loss/30的变化为135.7824265877406, 115.17959363659223, 107.6128164947033, 104.84226465026538, 100.80487464269002, 99.63234821061293]，再将Learn_rate设为0.0003时Loss/30的变化为[153.36195262273154, 149.8807134906451, 136.98581780592602, 130.88118385076524, 121.36723393797874, 115.37153724829356]，最后将Learn_rate设为0.00003时Loss/30的变化为[153.50621078014373, 153.49108584721884, 153.52144017219544, 153.5047466357549, 153.3507780234019, 153.34422073364257]，为了比较方便将以上数据绘制成了折线图，并得出了本实验的最终结论。

## 运行结果
#### learning rate = 0.0005：

<img src="README.assets\image-20210924210725631.png" alt="image-20210924210725631" style="zoom: 80%;" />

#### learning rate = 0.001:

<img src="README.assets\image-20210924210742710.png" alt="image-20210924210742710" style="zoom:80%;" />

#### learning rate = 0.0015:

<img src="README.assets\image-20210924210829761.png" alt="image-20210924210829761" style="zoom:80%;" />

#### 不同学习率下的train_loss曲线

<img src="README.assets\image-20210925154105542.png" alt="image-20210925154105542" style="zoom:67%;" />

## 结果分析

        根据以上两个图可以知道​		
从不同学习率下的train_loss曲线图像可以看出，学习率最大的绿色曲线在一开始收敛速度是最快的，而学习率最小的蓝色曲线收敛的速度最慢。但是随着epoch数的增加，学习率最大的绿色曲线很快在loss较大的地方收敛了，学习率最小的曲线在loss较小的地方才刚开始收敛，最终导致学习率大的准确率较低，而学习率小的准确率更高。

​		和我们课上所学的知识一致：学习率太大会很难逼近最优值，但是其收敛速度是很快的，如果离最优点较远，其可以更快的逼近最优点。最好的方法是引入动态学习率，在一开始采用较大的学习率快速逼近loss函数的最低值，然后在最低值附近变成较小的学习率，防止错过波谷，更好的逼近最优值，这样在效率和质量上都可以达到比较好的效果。



